{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TCG Card Embedding Training (Fixed)\n",
        "\n",
        "Train FastViT-T12 embedding model with ArcFace loss for card recognition.\n",
        "\n",
        "**Features:**\n",
        "- Heavy augmentation for single-sample-per-class learning\n",
        "- Multi-view training (4 augmented views per card)\n",
        "- Memory bank for cross-batch hard negative mining\n",
        "- Recall@K validation metrics\n",
        "\n",
        "**Prerequisites:**\n",
        "- Card images organized by class in Google Drive\n",
        "\n",
        "**Estimated Time:** ~10-12 hours for full training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch torchvision --upgrade\n",
        "!pip install -q timm lightning pytorch-metric-learning\n",
        "!pip install -q albumentations pyyaml tqdm pillow\n",
        "!pip install -q wandb annoy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set paths\n",
        "DRIVE_PROJECT = '/content/drive/MyDrive/tcg-scanner'\n",
        "WORK_DIR = '/content/tcg-scanner'\n",
        "\n",
        "import os\n",
        "os.makedirs(WORK_DIR, exist_ok=True)\n",
        "os.chdir(WORK_DIR)\n",
        "print(f\"Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify data exists\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=== Checking paths ===\")\n",
        "drive_path = Path(DRIVE_PROJECT)\n",
        "print(f\"Drive project exists: {drive_path.exists()}\")\n",
        "\n",
        "images_path = drive_path / 'ml/data/images/riftbound'\n",
        "print(f\"Images path exists: {images_path.exists()}\")\n",
        "\n",
        "if images_path.exists():\n",
        "    groups = list(images_path.iterdir())\n",
        "    total_images = sum(len(list(g.glob('*.jpg'))) for g in groups if g.is_dir())\n",
        "    print(f\"Found {len(groups)} group folders with {total_images} total images\")\n",
        "\n",
        "manifest_path = drive_path / 'ml/data/processed/riftbound/training_manifest.json'\n",
        "print(f\"Manifest exists: {manifest_path.exists()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prepare Data\n",
        "\n",
        "Organize card images into class directories (one folder per card)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "import json\n",
        "import random\n",
        "\n",
        "# Source data\n",
        "cards_src = Path(DRIVE_PROJECT) / 'ml/data/images/riftbound'\n",
        "manifest_path = Path(DRIVE_PROJECT) / 'ml/data/processed/riftbound/training_manifest.json'\n",
        "\n",
        "# Load manifest\n",
        "if manifest_path.exists():\n",
        "    with open(manifest_path) as f:\n",
        "        manifest = json.load(f)\n",
        "    print(f\"Loaded manifest with {len(manifest)} cards\")\n",
        "else:\n",
        "    print(\"Manifest not found - using directory structure\")\n",
        "    manifest = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_embedding_dataset(src_dir, output_dir, manifest=None, train_ratio=0.85):\n",
        "    \"\"\"Prepare dataset in class-folder structure for embedding training.\"\"\"\n",
        "    src_dir = Path(src_dir)\n",
        "    output_dir = Path(output_dir)\n",
        "    \n",
        "    train_dir = output_dir / 'train'\n",
        "    val_dir = output_dir / 'val'\n",
        "    \n",
        "    # Collect all images\n",
        "    cards = []\n",
        "    missing_count = 0\n",
        "    \n",
        "    if manifest:\n",
        "        # Use manifest for proper card-to-image mapping\n",
        "        for entry in manifest:\n",
        "            # FIX: Replace Windows backslashes with forward slashes for Linux\n",
        "            img_rel_path = entry['image_path'].replace('\\\\', '/')\n",
        "            img_path = Path(DRIVE_PROJECT) / 'ml/data' / img_rel_path\n",
        "            \n",
        "            if img_path.exists():\n",
        "                cards.append({\n",
        "                    'path': img_path,\n",
        "                    'id': str(entry['product_id']),\n",
        "                    'name': entry['clean_name']\n",
        "                })\n",
        "            else:\n",
        "                missing_count += 1\n",
        "                if missing_count <= 5:\n",
        "                    print(f\"Missing: {img_path}\")\n",
        "    else:\n",
        "        # Fall back to directory structure\n",
        "        for img_path in src_dir.rglob('*.jpg'):\n",
        "            card_id = img_path.stem.split('_')[0]\n",
        "            cards.append({\n",
        "                'path': img_path,\n",
        "                'id': card_id,\n",
        "                'name': img_path.stem\n",
        "            })\n",
        "    \n",
        "    print(f\"Found {len(cards)} card images\")\n",
        "    if missing_count > 0:\n",
        "        print(f\"Warning: {missing_count} images from manifest were not found\")\n",
        "    \n",
        "    if len(cards) == 0:\n",
        "        raise ValueError(\"No cards found! Check your paths.\")\n",
        "    \n",
        "    # Shuffle and split\n",
        "    random.seed(42)\n",
        "    random.shuffle(cards)\n",
        "    \n",
        "    split_idx = int(len(cards) * train_ratio)\n",
        "    train_cards = cards[:split_idx]\n",
        "    val_cards = cards[split_idx:]\n",
        "    \n",
        "    print(f\"Train: {len(train_cards)} cards, Val: {len(val_cards)} cards\")\n",
        "    \n",
        "    # Create directories and copy files\n",
        "    for split_name, split_cards, split_dir in [\n",
        "        ('train', train_cards, train_dir),\n",
        "        ('val', val_cards, val_dir)\n",
        "    ]:\n",
        "        print(f\"Copying {split_name} images...\")\n",
        "        for card in split_cards:\n",
        "            class_dir = split_dir / card['id']\n",
        "            class_dir.mkdir(parents=True, exist_ok=True)\n",
        "            \n",
        "            # Sanitize filename (remove special chars)\n",
        "            safe_name = \"\".join(c for c in card['name'] if c.isalnum() or c in ' _-').strip()\n",
        "            dst_path = class_dir / f\"{safe_name}.jpg\"\n",
        "            if not dst_path.exists():\n",
        "                shutil.copy(card['path'], dst_path)\n",
        "    \n",
        "    # Count classes\n",
        "    train_classes = len(list(train_dir.iterdir()))\n",
        "    val_classes = len(list(val_dir.iterdir()))\n",
        "    \n",
        "    print(f\"\\nDataset prepared:\")\n",
        "    print(f\"  Train: {train_classes} classes in {train_dir}\")\n",
        "    print(f\"  Val: {val_classes} classes in {val_dir}\")\n",
        "    \n",
        "    return train_dir, val_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare dataset\n",
        "train_dir, val_dir = prepare_embedding_dataset(\n",
        "    src_dir=cards_src,\n",
        "    output_dir=Path(WORK_DIR) / 'data/embedding',\n",
        "    manifest=manifest,\n",
        "    train_ratio=0.85\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Define Model and Training Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "import lightning as L\n",
        "import numpy as np\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from PIL import Image\n",
        "from pytorch_metric_learning import losses, miners\n",
        "from torch.utils.data import DataLoader, Dataset, Sampler\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    'model': {\n",
        "        'backbone': 'fastvit_t12',\n",
        "        'embedding_dim': 384,\n",
        "        'dropout': 0.2,\n",
        "    },\n",
        "    'training': {\n",
        "        'epochs': 100,\n",
        "        'batch_size': 64,\n",
        "        'learning_rate': 0.0003,\n",
        "        'weight_decay': 0.05,\n",
        "        'views_per_card': 4,\n",
        "    },\n",
        "    'metric_learning': {\n",
        "        'margin': 0.5,\n",
        "        'scale': 64,\n",
        "        'mining_margin': 0.3,\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_train_transforms():\n",
        "    \"\"\"Heavy augmentation pipeline for training.\"\"\"\n",
        "    return A.Compose([\n",
        "        # Geometric\n",
        "        A.Perspective(scale=(0.05, 0.15), p=0.5),\n",
        "        A.Affine(rotate=(-20, 20), shear=(-10, 10), scale=(0.8, 1.2), p=0.8),\n",
        "        \n",
        "        # Lighting\n",
        "        A.OneOf([\n",
        "            A.RandomBrightnessContrast(brightness_limit=0.4, contrast_limit=0.4, p=1.0),\n",
        "            A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=30, val_shift_limit=30, p=1.0),\n",
        "            A.RandomGamma(gamma_limit=(60, 140), p=1.0),\n",
        "        ], p=0.7),\n",
        "        \n",
        "        # Shadows\n",
        "        A.RandomShadow(shadow_roi=(0, 0, 1, 1), p=0.3),\n",
        "        \n",
        "        # Blur\n",
        "        A.OneOf([\n",
        "            A.GaussianBlur(blur_limit=(3, 7), p=1.0),\n",
        "            A.MotionBlur(blur_limit=7, p=1.0),\n",
        "        ], p=0.4),\n",
        "        \n",
        "        # Noise\n",
        "        A.OneOf([\n",
        "            A.GaussNoise(var_limit=(10, 80), p=1.0),\n",
        "            A.ISONoise(p=1.0),\n",
        "        ], p=0.5),\n",
        "        \n",
        "        # Compression\n",
        "        A.ImageCompression(quality_lower=60, quality_upper=100, p=0.5),\n",
        "        \n",
        "        # Occlusion\n",
        "        A.CoarseDropout(max_holes=3, max_height=40, max_width=40, p=0.3),\n",
        "        \n",
        "        # Resize and normalize\n",
        "        A.Resize(256, 256),\n",
        "        A.RandomCrop(224, 224),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "def get_val_transforms():\n",
        "    \"\"\"Light augmentation for validation.\"\"\"\n",
        "    return A.Compose([\n",
        "        A.Resize(256, 256),\n",
        "        A.CenterCrop(224, 224),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiViewCardDataset(Dataset):\n",
        "    \"\"\"Dataset generating multiple augmented views per card.\"\"\"\n",
        "    \n",
        "    def __init__(self, root_dir, transform, views_per_card=4):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.transform = transform\n",
        "        self.views_per_card = views_per_card\n",
        "        self.samples = []\n",
        "        self.class_to_idx = {}\n",
        "        \n",
        "        for idx, class_dir in enumerate(sorted(self.root_dir.iterdir())):\n",
        "            if class_dir.is_dir():\n",
        "                self.class_to_idx[class_dir.name] = idx\n",
        "                for img_path in class_dir.glob('*.[jp][pn][g]'):\n",
        "                    self.samples.append((img_path, idx))\n",
        "        \n",
        "        print(f\"Loaded {len(self.samples)} cards, {len(self.class_to_idx)} classes\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.samples) * self.views_per_card\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        card_idx = idx // self.views_per_card\n",
        "        img_path, label = self.samples[card_idx]\n",
        "        \n",
        "        image = np.array(Image.open(img_path).convert('RGB'))\n",
        "        augmented = self.transform(image=image)\n",
        "        \n",
        "        return augmented['image'], label\n",
        "    \n",
        "    @property\n",
        "    def num_classes(self):\n",
        "        return len(self.class_to_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CardEmbeddingModel(nn.Module):\n",
        "    \"\"\"FastViT embedding model.\"\"\"\n",
        "    \n",
        "    def __init__(self, backbone='fastvit_t12', embedding_dim=384, dropout=0.2):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.backbone = timm.create_model(backbone, pretrained=True, num_classes=0)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            dummy = torch.randn(1, 3, 224, 224)\n",
        "            features = self.backbone(dummy)\n",
        "            feature_dim = features.shape[-1]\n",
        "        \n",
        "        self.embedding_head = nn.Sequential(\n",
        "            nn.Linear(feature_dim, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, embedding_dim),\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        embeddings = self.embedding_head(features)\n",
        "        return F.normalize(embeddings, p=2, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CardEmbeddingModule(L.LightningModule):\n",
        "    \"\"\"Lightning module for training.\"\"\"\n",
        "    \n",
        "    def __init__(self, config, num_classes):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.config = config\n",
        "        \n",
        "        self.model = CardEmbeddingModel(\n",
        "            backbone=config['model']['backbone'],\n",
        "            embedding_dim=config['model']['embedding_dim'],\n",
        "            dropout=config['model']['dropout'],\n",
        "        )\n",
        "        \n",
        "        self.loss_fn = losses.ArcFaceLoss(\n",
        "            num_classes=num_classes,\n",
        "            embedding_size=config['model']['embedding_dim'],\n",
        "            margin=config['metric_learning']['margin'],\n",
        "            scale=config['metric_learning']['scale'],\n",
        "        )\n",
        "        \n",
        "        self.miner = miners.TripletMarginMiner(\n",
        "            margin=config['metric_learning']['mining_margin'],\n",
        "            type_of_triplets='hard',\n",
        "        )\n",
        "        \n",
        "        self.val_embeddings = []\n",
        "        self.val_labels = []\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, labels = batch\n",
        "        embeddings = self(images)\n",
        "        hard_pairs = self.miner(embeddings, labels)\n",
        "        loss = self.loss_fn(embeddings, labels)\n",
        "        \n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        images, labels = batch\n",
        "        embeddings = self(images)\n",
        "        loss = self.loss_fn(embeddings, labels)\n",
        "        \n",
        "        self.val_embeddings.append(embeddings.detach().cpu())\n",
        "        self.val_labels.append(labels.detach().cpu())\n",
        "        \n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "    \n",
        "    def on_validation_epoch_end(self):\n",
        "        if not self.val_embeddings:\n",
        "            return\n",
        "        \n",
        "        embeddings = torch.cat(self.val_embeddings, dim=0).numpy()\n",
        "        labels = torch.cat(self.val_labels, dim=0).numpy()\n",
        "        \n",
        "        # Calculate Recall@1\n",
        "        distances = np.linalg.norm(embeddings[:, None] - embeddings[None, :], axis=2)\n",
        "        np.fill_diagonal(distances, np.inf)\n",
        "        \n",
        "        nearest = np.argmin(distances, axis=1)\n",
        "        recall_1 = np.mean(labels[nearest] == labels)\n",
        "        \n",
        "        # Recall@5\n",
        "        nearest_5 = np.argsort(distances, axis=1)[:, :5]\n",
        "        recall_5 = np.mean([labels[i] in labels[nearest_5[i]] for i in range(len(labels))])\n",
        "        \n",
        "        self.log('val_recall_at_1', recall_1, prog_bar=True)\n",
        "        self.log('val_recall_at_5', recall_5, prog_bar=True)\n",
        "        \n",
        "        self.val_embeddings = []\n",
        "        self.val_labels = []\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            self.parameters(),\n",
        "            lr=self.config['training']['learning_rate'],\n",
        "            weight_decay=self.config['training']['weight_decay'],\n",
        "        )\n",
        "        \n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer,\n",
        "            T_max=self.config['training']['epochs'],\n",
        "        )\n",
        "        \n",
        "        return {'optimizer': optimizer, 'lr_scheduler': {'scheduler': scheduler, 'interval': 'epoch'}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "train_dataset = MultiViewCardDataset(\n",
        "    train_dir,\n",
        "    transform=get_train_transforms(),\n",
        "    views_per_card=CONFIG['training']['views_per_card'],\n",
        ")\n",
        "\n",
        "val_dataset = MultiViewCardDataset(\n",
        "    val_dir,\n",
        "    transform=get_val_transforms(),\n",
        "    views_per_card=1,\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining: {len(train_dataset)} samples ({len(train_dataset) // CONFIG['training']['views_per_card']} cards x {CONFIG['training']['views_per_card']} views)\")\n",
        "print(f\"Validation: {len(val_dataset)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataloaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=CONFIG['training']['batch_size'],\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=CONFIG['training']['batch_size'],\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model\n",
        "model = CardEmbeddingModule(CONFIG, num_classes=train_dataset.num_classes)\n",
        "\n",
        "# Callbacks\n",
        "callbacks = [\n",
        "    L.pytorch.callbacks.ModelCheckpoint(\n",
        "        dirpath=Path(WORK_DIR) / 'runs/embedding/checkpoints',\n",
        "        filename='best-{epoch}-{val_recall_at_1:.4f}',\n",
        "        monitor='val_recall_at_1',\n",
        "        mode='max',\n",
        "        save_top_k=3,\n",
        "        save_last=True,\n",
        "    ),\n",
        "    L.pytorch.callbacks.EarlyStopping(\n",
        "        monitor='val_recall_at_1',\n",
        "        patience=15,\n",
        "        mode='max',\n",
        "    ),\n",
        "    L.pytorch.callbacks.LearningRateMonitor(logging_interval='epoch'),\n",
        "]\n",
        "\n",
        "# Trainer\n",
        "trainer = L.Trainer(\n",
        "    max_epochs=CONFIG['training']['epochs'],\n",
        "    accelerator='auto',\n",
        "    precision='16-mixed',\n",
        "    callbacks=callbacks,\n",
        "    default_root_dir=Path(WORK_DIR) / 'runs/embedding',\n",
        "    log_every_n_steps=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train!\n",
        "print(\"Starting training...\")\n",
        "print(f\"Training {train_dataset.num_classes} card classes\")\n",
        "print(f\"Batch size: {CONFIG['training']['batch_size']}\")\n",
        "print(f\"Max epochs: {CONFIG['training']['epochs']}\")\n",
        "print(\"\")\n",
        "trainer.fit(model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final model\n",
        "output_dir = Path(WORK_DIR) / 'runs/embedding'\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "torch.save(model.model.state_dict(), output_dir / 'final_model.pt')\n",
        "print(f\"Model saved to {output_dir / 'final_model.pt'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Generate Embeddings for All Cards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate embeddings for ALL cards (train + val) for the vector index\n",
        "model.eval()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Create a simple dataset for all cards\n",
        "all_cards_dir = Path(WORK_DIR) / 'data/embedding'\n",
        "\n",
        "all_embeddings = []\n",
        "all_labels = []\n",
        "all_product_ids = []\n",
        "\n",
        "val_transform = get_val_transforms()\n",
        "\n",
        "# Process train cards\n",
        "print(\"Generating embeddings for train cards...\")\n",
        "for class_dir in tqdm(sorted((all_cards_dir / 'train').iterdir())):\n",
        "    if class_dir.is_dir():\n",
        "        product_id = class_dir.name\n",
        "        for img_path in class_dir.glob('*.jpg'):\n",
        "            image = np.array(Image.open(img_path).convert('RGB'))\n",
        "            augmented = val_transform(image=image)\n",
        "            img_tensor = augmented['image'].unsqueeze(0).to(device)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                emb = model(img_tensor).cpu().numpy()[0]\n",
        "            \n",
        "            all_embeddings.append(emb)\n",
        "            all_product_ids.append(product_id)\n",
        "\n",
        "# Process val cards\n",
        "print(\"Generating embeddings for val cards...\")\n",
        "for class_dir in tqdm(sorted((all_cards_dir / 'val').iterdir())):\n",
        "    if class_dir.is_dir():\n",
        "        product_id = class_dir.name\n",
        "        for img_path in class_dir.glob('*.jpg'):\n",
        "            image = np.array(Image.open(img_path).convert('RGB'))\n",
        "            augmented = val_transform(image=image)\n",
        "            img_tensor = augmented['image'].unsqueeze(0).to(device)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                emb = model(img_tensor).cpu().numpy()[0]\n",
        "            \n",
        "            all_embeddings.append(emb)\n",
        "            all_product_ids.append(product_id)\n",
        "\n",
        "embeddings = np.array(all_embeddings)\n",
        "print(f\"\\nGenerated {len(embeddings)} embeddings with shape {embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate final metrics on validation set\n",
        "print(\"\\n=== Validation Metrics ===\")\n",
        "\n",
        "# Use only val embeddings for metrics\n",
        "val_start_idx = len(list((all_cards_dir / 'train').iterdir()))\n",
        "val_embeddings = embeddings[val_start_idx:]\n",
        "val_ids = all_product_ids[val_start_idx:]\n",
        "\n",
        "distances = np.linalg.norm(val_embeddings[:, None] - val_embeddings[None, :], axis=2)\n",
        "np.fill_diagonal(distances, np.inf)\n",
        "\n",
        "# Convert product_ids to numeric labels for comparison\n",
        "unique_ids = list(set(val_ids))\n",
        "id_to_label = {id_: i for i, id_ in enumerate(unique_ids)}\n",
        "val_labels = np.array([id_to_label[id_] for id_ in val_ids])\n",
        "\n",
        "for k in [1, 5, 10]:\n",
        "    nearest_k = np.argsort(distances, axis=1)[:, :k]\n",
        "    recall_k = np.mean([val_labels[i] in val_labels[nearest_k[i]] for i in range(len(val_labels))])\n",
        "    print(f\"Recall@{k}: {recall_k:.4f} ({recall_k*100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy model to Drive\n",
        "drive_models = Path(DRIVE_PROJECT) / 'models/embedding'\n",
        "drive_models.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "shutil.copy(output_dir / 'final_model.pt', drive_models / 'final_model.pt')\n",
        "print(f\"Model saved to Drive: {drive_models / 'final_model.pt'}\")\n",
        "\n",
        "# Save embeddings for vector index\n",
        "embeddings_dir = Path(DRIVE_PROJECT) / 'ml/data/embeddings'\n",
        "embeddings_dir.mkdir(parents=True, exist_ok=True)\n",
        "np.save(embeddings_dir / 'riftbound_embeddings.npy', embeddings)\n",
        "print(f\"Embeddings saved: {embeddings_dir / 'riftbound_embeddings.npy'}\")\n",
        "\n",
        "# Save product ID mapping\n",
        "import json\n",
        "with open(embeddings_dir / 'riftbound_product_ids.json', 'w') as f:\n",
        "    json.dump(all_product_ids, f)\n",
        "print(f\"Product IDs saved: {embeddings_dir / 'riftbound_product_ids.json'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Build Vector Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from annoy import AnnoyIndex\n",
        "\n",
        "# Build index\n",
        "embedding_dim = embeddings.shape[1]\n",
        "index = AnnoyIndex(embedding_dim, 'angular')\n",
        "\n",
        "for i, emb in enumerate(embeddings):\n",
        "    index.add_item(i, emb)\n",
        "\n",
        "print(f\"Building index with {len(embeddings)} vectors...\")\n",
        "index.build(10)  # 10 trees\n",
        "\n",
        "# Save index\n",
        "index_dir = Path(DRIVE_PROJECT) / 'models/indices'\n",
        "index_dir.mkdir(parents=True, exist_ok=True)\n",
        "index.save(str(index_dir / 'riftbound.ann'))\n",
        "print(f\"Index saved: {index_dir / 'riftbound.ann'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test index\n",
        "print(\"\\n=== Testing Vector Index ===\")\n",
        "test_idx = 0\n",
        "neighbors, distances = index.get_nns_by_item(test_idx, 5, include_distances=True)\n",
        "\n",
        "print(f\"Query card product_id: {all_product_ids[test_idx]}\")\n",
        "print(f\"Top 5 neighbors:\")\n",
        "for i, (n, d) in enumerate(zip(neighbors, distances)):\n",
        "    match = \"MATCH\" if all_product_ids[n] == all_product_ids[test_idx] else \"\"\n",
        "    print(f\"  {i+1}. Product ID: {all_product_ids[n]}, Distance: {d:.4f} {match}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Export to TFLite (for Flutter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export to ONNX first, then to TFLite\n",
        "import torch.onnx\n",
        "\n",
        "# Load the model for export\n",
        "export_model = CardEmbeddingModel(\n",
        "    backbone=CONFIG['model']['backbone'],\n",
        "    embedding_dim=CONFIG['model']['embedding_dim'],\n",
        "    dropout=0.0,  # Disable dropout for inference\n",
        ")\n",
        "export_model.load_state_dict(torch.load(output_dir / 'final_model.pt'))\n",
        "export_model.eval()\n",
        "\n",
        "# Export to ONNX\n",
        "dummy_input = torch.randn(1, 3, 224, 224)\n",
        "onnx_path = output_dir / 'embedding.onnx'\n",
        "\n",
        "torch.onnx.export(\n",
        "    export_model,\n",
        "    dummy_input,\n",
        "    onnx_path,\n",
        "    export_params=True,\n",
        "    opset_version=12,\n",
        "    input_names=['input'],\n",
        "    output_names=['embedding'],\n",
        "    dynamic_axes={'input': {0: 'batch_size'}, 'embedding': {0: 'batch_size'}}\n",
        ")\n",
        "print(f\"ONNX model saved: {onnx_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert ONNX to TFLite\n",
        "!pip install -q onnx2tf tensorflow\n",
        "\n",
        "import subprocess\n",
        "tflite_dir = output_dir / 'tflite'\n",
        "tflite_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Convert using onnx2tf\n",
        "!onnx2tf -i {onnx_path} -o {tflite_dir} -oiqt\n",
        "\n",
        "print(f\"TFLite models saved to: {tflite_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy TFLite to Drive\n",
        "import glob\n",
        "\n",
        "tflite_files = list(tflite_dir.glob('*.tflite'))\n",
        "if tflite_files:\n",
        "    # Copy the integer quantized version if available, otherwise the float version\n",
        "    for tf_file in tflite_files:\n",
        "        shutil.copy(tf_file, drive_models / tf_file.name)\n",
        "        print(f\"Copied: {tf_file.name}\")\n",
        "else:\n",
        "    print(\"No TFLite files found. You may need to convert manually.\")\n",
        "\n",
        "print(f\"\\n=== All models saved to Drive ===\")\n",
        "print(f\"Location: {drive_models}\")\n",
        "for f in drive_models.iterdir():\n",
        "    print(f\"  - {f.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Done!\n",
        "\n",
        "Your trained models are now saved to Google Drive:\n",
        "- `models/embedding/final_model.pt` - PyTorch model\n",
        "- `models/embedding/*.tflite` - TFLite model for Flutter\n",
        "- `models/indices/riftbound.ann` - Annoy vector index\n",
        "- `ml/data/embeddings/riftbound_embeddings.npy` - All card embeddings\n",
        "- `ml/data/embeddings/riftbound_product_ids.json` - Product ID mapping"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
