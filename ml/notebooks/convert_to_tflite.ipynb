{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCG Scanner - Model Conversion to TFLite\n",
    "\n",
    "This notebook converts trained PyTorch models to TFLite format for Flutter mobile deployment.\n",
    "\n",
    "**Models to convert:**\n",
    "1. YOLOv8 Detection Model (best.pt) -> detection.tflite\n",
    "2. FastViT Embedding Model (embedding_model.pt) -> embedding.tflite\n",
    "\n",
    "**Requirements:**\n",
    "- Trained models in Google Drive\n",
    "- T4 GPU runtime (recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q ultralytics>=8.0.0\n",
    "!pip install -q timm>=0.9.12\n",
    "!pip install -q onnx>=1.14.0\n",
    "!pip install -q onnxruntime>=1.16.0\n",
    "!pip install -q onnx-tf>=1.10.0\n",
    "!pip install -q tf2onnx>=1.14.0\n",
    "\n",
    "print(\"Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set paths\n",
    "DRIVE_ROOT = Path('/content/drive/MyDrive/tcg-scanner')\n",
    "MODELS_DIR = DRIVE_ROOT / 'models'\n",
    "OUTPUT_DIR = DRIVE_ROOT / 'models/tflite'\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Drive root: {DRIVE_ROOT}\")\n",
    "print(f\"Models dir: {MODELS_DIR}\")\n",
    "print(f\"Output dir: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify model files exist\n",
    "detection_model = MODELS_DIR / 'detection/best.pt'\n",
    "embedding_model = MODELS_DIR / 'embedding/embedding_model.pt'\n",
    "\n",
    "print(\"Checking model files...\")\n",
    "print(f\"Detection model: {detection_model.exists()} - {detection_model}\")\n",
    "print(f\"Embedding model: {embedding_model.exists()} - {embedding_model}\")\n",
    "\n",
    "if not detection_model.exists():\n",
    "    print(\"\\nWARNING: Detection model not found!\")\n",
    "    print(\"Expected path: models/detection/best.pt\")\n",
    "    \n",
    "if not embedding_model.exists():\n",
    "    print(\"\\nWARNING: Embedding model not found!\")\n",
    "    print(\"Expected path: models/embedding/embedding_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convert Detection Model (YOLOv8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport timm\n\nprint(\"=\"*50)\nprint(\"Converting Embedding Model (FastViT)\")\nprint(\"=\"*50)\n\n# Define the model architecture (MUST match training exactly!)\nclass EmbeddingModel(nn.Module):\n    def __init__(self, backbone='fastvit_t12', embedding_dim=384, dropout=0.2):\n        super().__init__()\n        self.backbone = timm.create_model(backbone, pretrained=False, num_classes=0)\n\n        # Get feature dimension\n        with torch.no_grad():\n            dummy = torch.randn(1, 3, 224, 224)\n            features = self.backbone(dummy)\n            feature_dim = features.shape[-1]\n\n        # Must match training architecture exactly (includes Dropout!)\n        self.head = nn.Sequential(\n            nn.Linear(feature_dim, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout),  # This was missing before!\n            nn.Linear(512, embedding_dim),\n        )\n\n    def forward(self, x):\n        features = self.backbone(x)\n        embeddings = self.head(features)\n        return F.normalize(embeddings, p=2, dim=1)\n\n# Load trained weights\nprint(f\"\\nLoading model from: {embedding_model}\")\nmodel = EmbeddingModel()\nstate_dict = torch.load(embedding_model, map_location='cpu')\nmodel.load_state_dict(state_dict)\nmodel.eval()\n\nprint(\"Model loaded successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Find and copy the TFLite file\n",
    "# YOLO creates the tflite in a subfolder\n",
    "detection_output = OUTPUT_DIR / 'detection.tflite'\n",
    "\n",
    "# Search for the generated tflite file\n",
    "tflite_files = list(Path(export_path).parent.glob('**/*.tflite')) if export_path else []\n",
    "if not tflite_files:\n",
    "    tflite_files = list(MODELS_DIR.glob('**/best*.tflite'))\n",
    "\n",
    "if tflite_files:\n",
    "    src_file = tflite_files[0]\n",
    "    shutil.copy(src_file, detection_output)\n",
    "    size_mb = detection_output.stat().st_size / 1024 / 1024\n",
    "    print(f\"Detection model saved to: {detection_output}\")\n",
    "    print(f\"Size: {size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(\"ERROR: Could not find generated TFLite file\")\n",
    "    print(\"Searching in:\", MODELS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert Embedding Model (FastViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"Converting Embedding Model (FastViT)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define the model architecture (must match training)\n",
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, backbone='fastvit_t12', embedding_dim=384):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(backbone, pretrained=False, num_classes=0)\n",
    "        \n",
    "        # Get feature dimension\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.randn(1, 3, 224, 224)\n",
    "            features = self.backbone(dummy)\n",
    "            feature_dim = features.shape[-1]\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, embedding_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        embeddings = self.head(features)\n",
    "        return F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "# Load trained weights\n",
    "print(f\"\\nLoading model from: {embedding_model}\")\n",
    "model = EmbeddingModel()\n",
    "state_dict = torch.load(embedding_model, map_location='cpu')\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX first\n",
    "print(\"\\nExporting to ONNX...\")\n",
    "\n",
    "onnx_path = OUTPUT_DIR / 'embedding.onnx'\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    str(onnx_path),\n",
    "    export_params=True,\n",
    "    opset_version=12,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input'],\n",
    "    output_names=['embedding'],\n",
    "    dynamic_axes={\n",
    "        'input': {0: 'batch_size'},\n",
    "        'embedding': {0: 'batch_size'}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"ONNX model saved: {onnx_path}\")\n",
    "print(f\"Size: {onnx_path.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ONNX to TensorFlow SavedModel\n",
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "\n",
    "print(\"\\nConverting ONNX to TensorFlow...\")\n",
    "\n",
    "# Load ONNX model\n",
    "onnx_model = onnx.load(str(onnx_path))\n",
    "\n",
    "# Convert to TensorFlow\n",
    "tf_rep = prepare(onnx_model)\n",
    "tf_path = OUTPUT_DIR / 'embedding_tf'\n",
    "tf_rep.export_graph(str(tf_path))\n",
    "\n",
    "print(f\"TensorFlow model saved: {tf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert TensorFlow to TFLite\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"\\nConverting TensorFlow to TFLite...\")\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(str(tf_path))\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save TFLite model\n",
    "embedding_output = OUTPUT_DIR / 'embedding.tflite'\n",
    "with open(embedding_output, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "size_mb = embedding_output.stat().st_size / 1024 / 1024\n",
    "print(f\"\\nEmbedding model saved to: {embedding_output}\")\n",
    "print(f\"Size: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup intermediate files\n",
    "import shutil\n",
    "\n",
    "print(\"\\nCleaning up intermediate files...\")\n",
    "\n",
    "# Remove TensorFlow SavedModel directory\n",
    "if tf_path.exists():\n",
    "    shutil.rmtree(tf_path)\n",
    "    print(f\"Removed: {tf_path}\")\n",
    "\n",
    "# Optionally remove ONNX file (uncomment if you don't need it)\n",
    "# if onnx_path.exists():\n",
    "#     onnx_path.unlink()\n",
    "#     print(f\"Removed: {onnx_path}\")\n",
    "\n",
    "print(\"Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verify Converted Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"Conversion Summary\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\nOutput directory: {OUTPUT_DIR}\")\n",
    "print(\"\\nGenerated files:\")\n",
    "\n",
    "for f in OUTPUT_DIR.glob('*.tflite'):\n",
    "    size_mb = f.stat().st_size / 1024 / 1024\n",
    "    print(f\"  - {f.name}: {size_mb:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Next Steps:\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "1. Download the TFLite files from Google Drive:\n",
    "   - models/tflite/detection.tflite\n",
    "   - models/tflite/embedding.tflite\n",
    "\n",
    "2. Copy to your Flutter project:\n",
    "   - mobile/flutter/assets/models/detection.tflite\n",
    "   - mobile/flutter/assets/models/embedding.tflite\n",
    "\n",
    "3. Run Flutter app:\n",
    "   cd mobile/flutter\n",
    "   flutter pub get\n",
    "   flutter run\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Test TFLite models\n",
    "import numpy as np\n",
    "\n",
    "print(\"Testing TFLite models...\")\n",
    "\n",
    "# Test detection model\n",
    "if (OUTPUT_DIR / 'detection.tflite').exists():\n",
    "    interpreter = tf.lite.Interpreter(model_path=str(OUTPUT_DIR / 'detection.tflite'))\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    print(f\"\\nDetection model:\")\n",
    "    print(f\"  Input shape: {input_details[0]['shape']}\")\n",
    "    print(f\"  Input dtype: {input_details[0]['dtype']}\")\n",
    "    print(f\"  Output shapes: {[o['shape'] for o in output_details]}\")\n",
    "\n",
    "# Test embedding model\n",
    "if (OUTPUT_DIR / 'embedding.tflite').exists():\n",
    "    interpreter = tf.lite.Interpreter(model_path=str(OUTPUT_DIR / 'embedding.tflite'))\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    print(f\"\\nEmbedding model:\")\n",
    "    print(f\"  Input shape: {input_details[0]['shape']}\")\n",
    "    print(f\"  Input dtype: {input_details[0]['dtype']}\")\n",
    "    print(f\"  Output shape: {output_details[0]['shape']}\")\n",
    "    \n",
    "    # Run inference test\n",
    "    test_input = np.random.rand(1, 3, 224, 224).astype(np.float32)\n",
    "    interpreter.set_tensor(input_details[0]['index'], test_input)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[0]['index'])\n",
    "    print(f\"  Test output shape: {output.shape}\")\n",
    "    print(f\"  Embedding norm: {np.linalg.norm(output[0]):.4f} (should be ~1.0)\")\n",
    "\n",
    "print(\"\\nAll tests passed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}