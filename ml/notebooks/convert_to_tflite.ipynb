{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# TCG Scanner - Model Conversion to ONNX\n\nThis notebook converts trained PyTorch models to ONNX format for Flutter mobile deployment.\n\n**Models to convert:**\n1. YOLOv8 Detection Model (best.pt) \u2192 detection.onnx\n2. FastViT Embedding Model (embedding_model.pt) \u2192 embedding.onnx\n\n**Why ONNX instead of TFLite:**\n- \u2705 Better YOLOv8 performance on mobile\n- \u2705 Single runtime for both models (onnxruntime)\n- \u2705 No TensorFlow dependency issues\n- \u2705 Excellent Flutter support\n\n**Requirements:**\n- Trained models in Google Drive\n- No GPU needed"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Install required packages\nprint(\"Installing packages...\")\n\n# Core packages (no TensorFlow needed!)\n!pip install -q ultralytics>=8.0.0\n!pip install -q timm>=0.9.12\n!pip install -q onnx>=1.14.0\n!pip install -q onnxruntime>=1.16.0\n\nprint(\"\u2705 All packages installed!\")\nprint(\"\\n\ud83d\udce6 Export Format: ONNX for both models\")\nprint(\"   - Detection: YOLOv8 \u2192 ONNX\")\nprint(\"   - Embedding: FastViT \u2192 ONNX\")\nprint(\"   - Runtime: onnxruntime-flutter\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Set paths\n",
        "DRIVE_ROOT = Path('/content/drive/MyDrive/tcg-scanner')\n",
        "MODELS_DIR = DRIVE_ROOT / 'models'\n",
        "OUTPUT_DIR = DRIVE_ROOT / 'models/tflite'\n",
        "\n",
        "# Create output directory\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Drive root: {DRIVE_ROOT}\")\n",
        "print(f\"Models dir: {MODELS_DIR}\")\n",
        "print(f\"Output dir: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify model files exist\n",
        "detection_model_path = MODELS_DIR / 'detection/best.pt'\n",
        "embedding_model_path = MODELS_DIR / 'embedding/embedding_model.pt'\n",
        "\n",
        "print(\"Checking model files...\")\n",
        "print(f\"Detection model: {detection_model_path.exists()} - {detection_model_path}\")\n",
        "print(f\"Embedding model: {embedding_model_path.exists()} - {embedding_model_path}\")\n",
        "\n",
        "if not detection_model_path.exists():\n",
        "    print(\"\\nWARNING: Detection model not found!\")\n",
        "    print(\"Expected path: models/detection/best.pt\")\n",
        "    \n",
        "if not embedding_model_path.exists():\n",
        "    print(\"\\nWARNING: Embedding model not found!\")\n",
        "    print(\"Expected path: models/embedding/embedding_model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Convert Detection Model (YOLOv8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from ultralytics import YOLO\n\nprint(\"=\"*50)\nprint(\"Converting Detection Model (YOLOv8)\")\nprint(\"=\"*50)\n\n# Load the trained model\nprint(f\"\\nLoading model from: {detection_model_path}\")\nyolo_model = YOLO(str(detection_model_path))\n\n# Export to ONNX (better mobile support than TFLite for YOLO)\nprint(\"\\nExporting to ONNX (recommended for mobile YOLO)...\")\nexport_path = yolo_model.export(format='onnx', imgsz=640)\n\nprint(f\"\\n\u2705 Export successful: {export_path}\")\nprint(f\"   Format: ONNX (use onnxruntime-flutter)\")\nprint(f\"\\n\ud83d\udca1 ONNX is recommended for YOLOv8 on mobile:\")\nprint(f\"   - Better performance than TFLite for YOLO\")\nprint(f\"   - Official ONNX Runtime support in Flutter\")\nprint(f\"   - No TensorFlow compatibility issues\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Copy ONNX model to output directory\nimport shutil\n\ndetection_output = OUTPUT_DIR / 'detection.onnx'\n\nif export_path:\n    shutil.copy(str(export_path), str(detection_output))\n    size_mb = detection_output.stat().st_size / 1024 / 1024\n    print(f\"\u2705 Detection model saved to: {detection_output}\")\n    print(f\"   Size: {size_mb:.2f} MB\")\n    print(f\"\\n\ud83d\udce6 Flutter package: onnxruntime\")\n    print(f\"   Add to pubspec.yaml: onnxruntime: ^1.14.0\")\nelse:\n    print(\"\u274c ERROR: Export failed\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<cell_type>markdown</cell_type>## 3. Convert Embedding Model (FastViT)\n\n**Method**: Using ai-edge-torch (Google's official PyTorch\u2192TFLite converter)\n\nThis is the modern, recommended approach for PyTorch to TFLite conversion."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"Converting Embedding Model (FastViT)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Define the model architecture (MUST match training exactly!)\n",
        "class EmbeddingModel(nn.Module):\n",
        "    def __init__(self, backbone='fastvit_t12', embedding_dim=384, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(backbone, pretrained=False, num_classes=0)\n",
        "\n",
        "        # Get feature dimension\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.randn(1, 3, 224, 224)\n",
        "            features = self.backbone(dummy)\n",
        "            feature_dim = features.shape[-1]\n",
        "\n",
        "        # Must match training architecture exactly (includes Dropout!)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(feature_dim, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, embedding_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        embeddings = self.head(features)\n",
        "        return F.normalize(embeddings, p=2, dim=1)\n",
        "\n",
        "# Load trained weights\n",
        "print(f\"\\nLoading model from: {embedding_model_path}\")\n",
        "embed_model = EmbeddingModel()\n",
        "state_dict = torch.load(embedding_model_path, map_location='cpu')\n",
        "embed_model.load_state_dict(state_dict)\n",
        "embed_model.eval()\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Export Embedding Model to ONNX\nimport torch\n\nprint(\"=\"*50)\nprint(\"Converting Embedding Model to ONNX\")\nprint(\"=\"*50)\n\n# Prepare sample input\nsample_input = torch.randn(1, 3, 224, 224)\n\n# Export to ONNX\nonnx_output = OUTPUT_DIR / 'embedding.onnx'\n\nprint(\"\\nExporting to ONNX...\")\ntorch.onnx.export(\n    embed_model,\n    sample_input,\n    str(onnx_output),\n    export_params=True,\n    opset_version=13,\n    do_constant_folding=True,\n    input_names=['input'],\n    output_names=['embedding'],\n    dynamic_axes={\n        'input': {0: 'batch_size'},\n        'embedding': {0: 'batch_size'}\n    }\n)\n\nsize_mb = onnx_output.stat().st_size / 1024 / 1024\nprint(f\"\\n\u2705 Embedding model saved to: {onnx_output}\")\nprint(f\"   Size: {size_mb:.2f} MB\")\nprint(f\"   Format: ONNX\")\n\n# Test if normalized\nimport onnxruntime as ort\nimport numpy as np\n\nsession = ort.InferenceSession(str(onnx_output))\ntest_input = np.random.randn(1, 3, 224, 224).astype(np.float32)\noutput = session.run(None, {'input': test_input})[0]\nnorm = np.linalg.norm(output[0])\n\nprint(f\"\\n   Test embedding norm: {norm:.4f}\")\nif 0.95 < norm < 1.05:\n    print(f\"   \u2705 L2 normalized (norm ~1.0)\")\nelse:\n    print(f\"   \u26a0\ufe0f  Not normalized - add normalization in Flutter\")\n    print(f\"      embedding = embedding / ||embedding||\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "<cell_type>markdown</cell_type>## 4. Verify Converted Models & Flutter Integration"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"CONVERSION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nOutput directory: {OUTPUT_DIR}\")\n",
        "print(\"\\nGenerated ONNX models:\")\n",
        "\n",
        "for f in OUTPUT_DIR.glob('*.onnx'):\n",
        "    size_mb = f.stat().st_size / 1024 / 1024\n",
        "    print(f\"  \u2705 {f.name}: {size_mb:.2f} MB\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FLUTTER INTEGRATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\"\"\n",
        "1. Add to pubspec.yaml:\n",
        "   dependencies:\n",
        "     onnxruntime: ^1.14.0\n",
        "\n",
        "2. Copy models to Flutter:\n",
        "   - assets/models/detection.onnx\n",
        "   - assets/models/embedding.onnx\n",
        "\n",
        "3. Load models in Flutter:\n",
        "   ```dart\n",
        "   import 'package:onnxruntime/onnxruntime.dart';\n",
        "   \n",
        "   final detectionSession = OrtSession.fromAsset(\n",
        "     'assets/models/detection.onnx'\n",
        "   );\n",
        "   \n",
        "   final embeddingSession = OrtSession.fromAsset(\n",
        "     'assets/models/embedding.onnx'\n",
        "   );\n",
        "   ```\n",
        "\n",
        "4. Run inference:\n",
        "   ```dart\n",
        "   // Detection\n",
        "   final detections = await detectionSession.run([imageTensor]);\n",
        "   \n",
        "   // Embedding\n",
        "   final embedding = await embeddingSession.run([cardTensor]);\n",
        "   ```\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"\u2705 CONVERSION COMPLETE!\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}