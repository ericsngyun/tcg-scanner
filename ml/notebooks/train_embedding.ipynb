{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TCG Card Embedding Training\n",
        "\n",
        "Train FastViT-T12 embedding model with ArcFace loss for card recognition.\n",
        "\n",
        "**Features:**\n",
        "- Heavy augmentation for single-sample-per-class learning\n",
        "- Multi-view training (4 augmented views per card)\n",
        "- Memory bank for cross-batch hard negative mining\n",
        "- Recall@K validation metrics\n",
        "\n",
        "**Prerequisites:**\n",
        "- Card images organized by class in Google Drive\n",
        "\n",
        "**Estimated Time:** ~10-12 hours for full training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch torchvision --upgrade\n",
        "!pip install -q timm lightning pytorch-metric-learning\n",
        "!pip install -q albumentations pyyaml tqdm pillow\n",
        "!pip install -q wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set paths\n",
        "DRIVE_PROJECT = '/content/drive/MyDrive/tcg-scanner'\n",
        "WORK_DIR = '/content/tcg-scanner'\n",
        "\n",
        "import os\n",
        "os.makedirs(WORK_DIR, exist_ok=True)\n",
        "os.chdir(WORK_DIR)\n",
        "print(f\"Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Initialize Weights & Biases\n",
        "import wandb\n",
        "\n",
        "# Uncomment and run to enable wandb logging\n",
        "# wandb.login()\n",
        "# wandb.init(project='tcg-scanner', name='embedding-fastvit-t12')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prepare Data\n",
        "\n",
        "Organize card images into class directories (one folder per card)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "import json\n",
        "import random\n",
        "\n",
        "# Source data\n",
        "cards_src = Path(DRIVE_PROJECT) / 'ml/data/images/riftbound'\n",
        "manifest_path = Path(DRIVE_PROJECT) / 'ml/data/processed/riftbound/training_manifest.json'\n",
        "\n",
        "# Load manifest\n",
        "if manifest_path.exists():\n",
        "    with open(manifest_path) as f:\n",
        "        manifest = json.load(f)\n",
        "    print(f\"Loaded manifest with {len(manifest)} cards\")\n",
        "else:\n",
        "    print(\"Manifest not found - using directory structure\")\n",
        "    manifest = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_embedding_dataset(src_dir, output_dir, manifest=None, train_ratio=0.85):\n",
        "    \"\"\"Prepare dataset in class-folder structure for embedding training.\"\"\"\n",
        "    src_dir = Path(src_dir)\n",
        "    output_dir = Path(output_dir)\n",
        "    \n",
        "    train_dir = output_dir / 'train'\n",
        "    val_dir = output_dir / 'val'\n",
        "    \n",
        "    # Collect all images\n",
        "    if manifest:\n",
        "        # Use manifest for proper card-to-image mapping\n",
        "        cards = []\n",
        "        for entry in manifest:\n",
        "            img_path = Path(DRIVE_PROJECT) / 'ml/data' / entry['image_path']\n",
        "            if img_path.exists():\n",
        "                cards.append({\n",
        "                    'path': img_path,\n",
        "                    'id': str(entry['product_id']),\n",
        "                    'name': entry['clean_name']\n",
        "                })\n",
        "    else:\n",
        "        # Fall back to directory structure\n",
        "        cards = []\n",
        "        for img_path in src_dir.rglob('*.jpg'):\n",
        "            card_id = img_path.stem.split('_')[0]\n",
        "            cards.append({\n",
        "                'path': img_path,\n",
        "                'id': card_id,\n",
        "                'name': img_path.stem\n",
        "            })\n",
        "    \n",
        "    print(f\"Found {len(cards)} card images\")\n",
        "    \n",
        "    # Shuffle and split\n",
        "    random.seed(42)\n",
        "    random.shuffle(cards)\n",
        "    \n",
        "    split_idx = int(len(cards) * train_ratio)\n",
        "    train_cards = cards[:split_idx]\n",
        "    val_cards = cards[split_idx:]\n",
        "    \n",
        "    print(f\"Train: {len(train_cards)} cards, Val: {len(val_cards)} cards\")\n",
        "    \n",
        "    # Create directories and copy files\n",
        "    for split_name, split_cards, split_dir in [\n",
        "        ('train', train_cards, train_dir),\n",
        "        ('val', val_cards, val_dir)\n",
        "    ]:\n",
        "        for card in split_cards:\n",
        "            class_dir = split_dir / card['id']\n",
        "            class_dir.mkdir(parents=True, exist_ok=True)\n",
        "            \n",
        "            dst_path = class_dir / f\"{card['name']}.jpg\"\n",
        "            if not dst_path.exists():\n",
        "                shutil.copy(card['path'], dst_path)\n",
        "    \n",
        "    # Count classes\n",
        "    train_classes = len(list(train_dir.iterdir()))\n",
        "    val_classes = len(list(val_dir.iterdir()))\n",
        "    \n",
        "    print(f\"\\nDataset prepared:\")\n",
        "    print(f\"  Train: {train_classes} classes in {train_dir}\")\n",
        "    print(f\"  Val: {val_classes} classes in {val_dir}\")\n",
        "    \n",
        "    return train_dir, val_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare dataset\n",
        "train_dir, val_dir = prepare_embedding_dataset(\n",
        "    src_dir=cards_src,\n",
        "    output_dir=Path(WORK_DIR) / 'data/embedding',\n",
        "    manifest=manifest,\n",
        "    train_ratio=0.85\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Define Model and Training Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "import lightning as L\n",
        "import numpy as np\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from PIL import Image\n",
        "from pytorch_metric_learning import losses, miners\n",
        "from torch.utils.data import DataLoader, Dataset, Sampler\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    'model': {\n",
        "        'backbone': 'fastvit_t12',\n",
        "        'embedding_dim': 384,\n",
        "        'dropout': 0.2,\n",
        "    },\n",
        "    'training': {\n",
        "        'epochs': 100,\n",
        "        'batch_size': 64,\n",
        "        'learning_rate': 0.0003,\n",
        "        'weight_decay': 0.05,\n",
        "        'views_per_card': 4,\n",
        "    },\n",
        "    'metric_learning': {\n",
        "        'margin': 0.5,\n",
        "        'scale': 64,\n",
        "        'mining_margin': 0.3,\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_train_transforms():\n",
        "    \"\"\"Heavy augmentation pipeline for training.\"\"\"\n",
        "    return A.Compose([\n",
        "        # Geometric\n",
        "        A.Perspective(scale=(0.05, 0.15), p=0.5),\n",
        "        A.Affine(rotate=(-20, 20), shear=(-10, 10), scale=(0.8, 1.2), p=0.8),\n",
        "        \n",
        "        # Lighting\n",
        "        A.OneOf([\n",
        "            A.RandomBrightnessContrast(brightness_limit=0.4, contrast_limit=0.4, p=1.0),\n",
        "            A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=30, val_shift_limit=30, p=1.0),\n",
        "            A.RandomGamma(gamma_limit=(60, 140), p=1.0),\n",
        "        ], p=0.7),\n",
        "        \n",
        "        # Shadows\n",
        "        A.RandomShadow(shadow_roi=(0, 0, 1, 1), p=0.3),\n",
        "        \n",
        "        # Blur\n",
        "        A.OneOf([\n",
        "            A.GaussianBlur(blur_limit=(3, 7), p=1.0),\n",
        "            A.MotionBlur(blur_limit=7, p=1.0),\n",
        "        ], p=0.4),\n",
        "        \n",
        "        # Noise\n",
        "        A.OneOf([\n",
        "            A.GaussNoise(var_limit=(10, 80), p=1.0),\n",
        "            A.ISONoise(p=1.0),\n",
        "        ], p=0.5),\n",
        "        \n",
        "        # Compression\n",
        "        A.ImageCompression(quality_lower=60, quality_upper=100, p=0.5),\n",
        "        \n",
        "        # Occlusion\n",
        "        A.CoarseDropout(max_holes=3, max_height=40, max_width=40, p=0.3),\n",
        "        \n",
        "        # Resize and normalize\n",
        "        A.Resize(256, 256),\n",
        "        A.RandomCrop(224, 224),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "def get_val_transforms():\n",
        "    \"\"\"Light augmentation for validation.\"\"\"\n",
        "    return A.Compose([\n",
        "        A.Resize(256, 256),\n",
        "        A.CenterCrop(224, 224),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiViewCardDataset(Dataset):\n",
        "    \"\"\"Dataset generating multiple augmented views per card.\"\"\"\n",
        "    \n",
        "    def __init__(self, root_dir, transform, views_per_card=4):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.transform = transform\n",
        "        self.views_per_card = views_per_card\n",
        "        self.samples = []\n",
        "        self.class_to_idx = {}\n",
        "        \n",
        "        for idx, class_dir in enumerate(sorted(self.root_dir.iterdir())):\n",
        "            if class_dir.is_dir():\n",
        "                self.class_to_idx[class_dir.name] = idx\n",
        "                for img_path in class_dir.glob('*.[jp][pn][g]'):\n",
        "                    self.samples.append((img_path, idx))\n",
        "        \n",
        "        print(f\"Loaded {len(self.samples)} cards, {len(self.class_to_idx)} classes\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.samples) * self.views_per_card\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        card_idx = idx // self.views_per_card\n",
        "        img_path, label = self.samples[card_idx]\n",
        "        \n",
        "        image = np.array(Image.open(img_path).convert('RGB'))\n",
        "        augmented = self.transform(image=image)\n",
        "        \n",
        "        return augmented['image'], label\n",
        "    \n",
        "    @property\n",
        "    def num_classes(self):\n",
        "        return len(self.class_to_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CardEmbeddingModel(nn.Module):\n",
        "    \"\"\"FastViT embedding model.\"\"\"\n",
        "    \n",
        "    def __init__(self, backbone='fastvit_t12', embedding_dim=384, dropout=0.2):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.backbone = timm.create_model(backbone, pretrained=True, num_classes=0)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            dummy = torch.randn(1, 3, 224, 224)\n",
        "            features = self.backbone(dummy)\n",
        "            feature_dim = features.shape[-1]\n",
        "        \n",
        "        self.embedding_head = nn.Sequential(\n",
        "            nn.Linear(feature_dim, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, embedding_dim),\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        embeddings = self.embedding_head(features)\n",
        "        return F.normalize(embeddings, p=2, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CardEmbeddingModule(L.LightningModule):\n",
        "    \"\"\"Lightning module for training.\"\"\"\n",
        "    \n",
        "    def __init__(self, config, num_classes):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.config = config\n",
        "        \n",
        "        self.model = CardEmbeddingModel(\n",
        "            backbone=config['model']['backbone'],\n",
        "            embedding_dim=config['model']['embedding_dim'],\n",
        "            dropout=config['model']['dropout'],\n",
        "        )\n",
        "        \n",
        "        self.loss_fn = losses.ArcFaceLoss(\n",
        "            num_classes=num_classes,\n",
        "            embedding_size=config['model']['embedding_dim'],\n",
        "            margin=config['metric_learning']['margin'],\n",
        "            scale=config['metric_learning']['scale'],\n",
        "        )\n",
        "        \n",
        "        self.miner = miners.TripletMarginMiner(\n",
        "            margin=config['metric_learning']['mining_margin'],\n",
        "            type_of_triplets='hard',\n",
        "        )\n",
        "        \n",
        "        self.val_embeddings = []\n",
        "        self.val_labels = []\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, labels = batch\n",
        "        embeddings = self(images)\n",
        "        hard_pairs = self.miner(embeddings, labels)\n",
        "        loss = self.loss_fn(embeddings, labels)\n",
        "        \n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        images, labels = batch\n",
        "        embeddings = self(images)\n",
        "        loss = self.loss_fn(embeddings, labels)\n",
        "        \n",
        "        self.val_embeddings.append(embeddings.detach().cpu())\n",
        "        self.val_labels.append(labels.detach().cpu())\n",
        "        \n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "    \n",
        "    def on_validation_epoch_end(self):\n",
        "        if not self.val_embeddings:\n",
        "            return\n",
        "        \n",
        "        embeddings = torch.cat(self.val_embeddings, dim=0).numpy()\n",
        "        labels = torch.cat(self.val_labels, dim=0).numpy()\n",
        "        \n",
        "        # Calculate Recall@1\n",
        "        distances = np.linalg.norm(embeddings[:, None] - embeddings[None, :], axis=2)\n",
        "        np.fill_diagonal(distances, np.inf)\n",
        "        \n",
        "        nearest = np.argmin(distances, axis=1)\n",
        "        recall_1 = np.mean(labels[nearest] == labels)\n",
        "        \n",
        "        # Recall@5\n",
        "        nearest_5 = np.argsort(distances, axis=1)[:, :5]\n",
        "        recall_5 = np.mean([labels[i] in labels[nearest_5[i]] for i in range(len(labels))])\n",
        "        \n",
        "        self.log('val_recall_at_1', recall_1, prog_bar=True)\n",
        "        self.log('val_recall_at_5', recall_5, prog_bar=True)\n",
        "        \n",
        "        self.val_embeddings = []\n",
        "        self.val_labels = []\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            self.parameters(),\n",
        "            lr=self.config['training']['learning_rate'],\n",
        "            weight_decay=self.config['training']['weight_decay'],\n",
        "        )\n",
        "        \n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer,\n",
        "            T_max=self.config['training']['epochs'],\n",
        "        )\n",
        "        \n",
        "        return {'optimizer': optimizer, 'lr_scheduler': {'scheduler': scheduler, 'interval': 'epoch'}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "train_dataset = MultiViewCardDataset(\n",
        "    train_dir,\n",
        "    transform=get_train_transforms(),\n",
        "    views_per_card=CONFIG['training']['views_per_card'],\n",
        ")\n",
        "\n",
        "val_dataset = MultiViewCardDataset(\n",
        "    val_dir,\n",
        "    transform=get_val_transforms(),\n",
        "    views_per_card=1,\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining: {len(train_dataset)} samples\")\n",
        "print(f\"Validation: {len(val_dataset)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataloaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=CONFIG['training']['batch_size'],\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=CONFIG['training']['batch_size'],\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model\n",
        "model = CardEmbeddingModule(CONFIG, num_classes=train_dataset.num_classes)\n",
        "\n",
        "# Callbacks\n",
        "callbacks = [\n",
        "    L.pytorch.callbacks.ModelCheckpoint(\n",
        "        dirpath=Path(WORK_DIR) / 'runs/embedding/checkpoints',\n",
        "        filename='best-{epoch}-{val_recall_at_1:.4f}',\n",
        "        monitor='val_recall_at_1',\n",
        "        mode='max',\n",
        "        save_top_k=3,\n",
        "        save_last=True,\n",
        "    ),\n",
        "    L.pytorch.callbacks.EarlyStopping(\n",
        "        monitor='val_recall_at_1',\n",
        "        patience=15,\n",
        "        mode='max',\n",
        "    ),\n",
        "    L.pytorch.callbacks.LearningRateMonitor(logging_interval='epoch'),\n",
        "]\n",
        "\n",
        "# Trainer\n",
        "trainer = L.Trainer(\n",
        "    max_epochs=CONFIG['training']['epochs'],\n",
        "    accelerator='auto',\n",
        "    precision='16-mixed',\n",
        "    callbacks=callbacks,\n",
        "    default_root_dir=Path(WORK_DIR) / 'runs/embedding',\n",
        "    log_every_n_steps=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train!\n",
        "print(\"Starting training...\")\n",
        "trainer.fit(model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final model\n",
        "output_dir = Path(WORK_DIR) / 'runs/embedding'\n",
        "torch.save(model.model.state_dict(), output_dir / 'final_model.pt')\n",
        "print(f\"Model saved to {output_dir / 'final_model.pt'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate embeddings for all cards\n",
        "model.eval()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "all_embeddings = []\n",
        "all_labels = []\n",
        "all_paths = []\n",
        "\n",
        "val_simple = MultiViewCardDataset(val_dir, get_val_transforms(), views_per_card=1)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for img, label in tqdm(val_simple):\n",
        "        img = img.unsqueeze(0).to(device)\n",
        "        emb = model(img).cpu().numpy()[0]\n",
        "        all_embeddings.append(emb)\n",
        "        all_labels.append(label)\n",
        "\n",
        "embeddings = np.array(all_embeddings)\n",
        "labels = np.array(all_labels)\n",
        "\n",
        "print(f\"Generated {len(embeddings)} embeddings\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate final metrics\n",
        "distances = np.linalg.norm(embeddings[:, None] - embeddings[None, :], axis=2)\n",
        "np.fill_diagonal(distances, np.inf)\n",
        "\n",
        "for k in [1, 5, 10]:\n",
        "    nearest_k = np.argsort(distances, axis=1)[:, :k]\n",
        "    recall_k = np.mean([labels[i] in labels[nearest_k[i]] for i in range(len(labels))])\n",
        "    print(f\"Recall@{k}: {recall_k:.4f} ({recall_k*100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy model to Drive\n",
        "drive_models = Path(DRIVE_PROJECT) / 'models/embedding'\n",
        "drive_models.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "shutil.copy(output_dir / 'final_model.pt', drive_models / 'final_model.pt')\n",
        "print(f\"Model saved to Drive: {drive_models / 'final_model.pt'}\")\n",
        "\n",
        "# Save embeddings for vector index\n",
        "embeddings_dir = Path(DRIVE_PROJECT) / 'ml/data/embeddings'\n",
        "embeddings_dir.mkdir(parents=True, exist_ok=True)\n",
        "np.save(embeddings_dir / 'riftbound.npy', embeddings)\n",
        "print(f\"Embeddings saved: {embeddings_dir / 'riftbound.npy'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Build Vector Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q annoy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from annoy import AnnoyIndex\n",
        "\n",
        "# Build index\n",
        "embedding_dim = embeddings.shape[1]\n",
        "index = AnnoyIndex(embedding_dim, 'angular')\n",
        "\n",
        "for i, emb in enumerate(embeddings):\n",
        "    index.add_item(i, emb)\n",
        "\n",
        "index.build(10)  # 10 trees\n",
        "\n",
        "# Save index\n",
        "index_dir = Path(DRIVE_PROJECT) / 'models/indices'\n",
        "index_dir.mkdir(parents=True, exist_ok=True)\n",
        "index.save(str(index_dir / 'riftbound.ann'))\n",
        "print(f\"Index saved: {index_dir / 'riftbound.ann'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test index\n",
        "test_idx = 0\n",
        "neighbors, distances = index.get_nns_by_item(test_idx, 5, include_distances=True)\n",
        "\n",
        "print(f\"Query card label: {labels[test_idx]}\")\n",
        "print(f\"Top 5 neighbors:\")\n",
        "for i, (n, d) in enumerate(zip(neighbors, distances)):\n",
        "    print(f\"  {i+1}. Label: {labels[n]}, Distance: {d:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
