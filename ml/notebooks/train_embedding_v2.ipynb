{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TCG Card Embedding Training v2\n",
        "\n",
        "Train FastViT-T12 embedding model for card recognition.\n",
        "\n",
        "**What this notebook does:**\n",
        "1. Prepares your 673 card images for training\n",
        "2. Trains an embedding model with ArcFace loss\n",
        "3. Generates embeddings for all cards\n",
        "4. Builds an Annoy vector search index\n",
        "5. Saves everything to Google Drive\n",
        "\n",
        "**Output files (saved to Drive):**\n",
        "- `models/embedding/final_model.pt` - PyTorch model\n",
        "- `models/indices/riftbound.ann` - Vector search index\n",
        "- `ml/data/embeddings/riftbound_embeddings.npy` - All embeddings\n",
        "- `ml/data/embeddings/riftbound_product_ids.json` - Product ID mapping\n",
        "\n",
        "**Estimated Time:** 6-12 hours (depends on early stopping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU - MUST be T4 or better\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install ALL dependencies upfront\n",
        "!pip install -q torch torchvision --upgrade\n",
        "!pip install -q timm==0.9.12\n",
        "!pip install -q lightning==2.1.0\n",
        "!pip install -q pytorch-metric-learning==2.3.0\n",
        "!pip install -q albumentations==1.3.1\n",
        "!pip install -q annoy==1.17.3\n",
        "!pip install -q pyyaml tqdm pillow\n",
        "\n",
        "print(\"\\n‚úÖ All dependencies installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set paths\n",
        "DRIVE_PROJECT = '/content/drive/MyDrive/tcg-scanner'\n",
        "WORK_DIR = '/content/tcg-scanner'\n",
        "\n",
        "import os\n",
        "os.makedirs(WORK_DIR, exist_ok=True)\n",
        "os.chdir(WORK_DIR)\n",
        "print(f\"Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify data exists before proceeding\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"VERIFYING DATA FILES\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "drive_path = Path(DRIVE_PROJECT)\n",
        "images_path = drive_path / 'ml/data/images/riftbound'\n",
        "manifest_path = drive_path / 'ml/data/processed/riftbound/training_manifest.json'\n",
        "\n",
        "# Check images\n",
        "if images_path.exists():\n",
        "    groups = [g for g in images_path.iterdir() if g.is_dir()]\n",
        "    total_images = sum(len(list(g.glob('*.jpg'))) for g in groups)\n",
        "    print(f\"‚úÖ Images: {total_images} cards in {len(groups)} groups\")\n",
        "else:\n",
        "    print(f\"‚ùå Images not found at: {images_path}\")\n",
        "    raise FileNotFoundError(\"Upload your images to Google Drive first!\")\n",
        "\n",
        "# Check manifest\n",
        "if manifest_path.exists():\n",
        "    print(f\"‚úÖ Manifest: {manifest_path.name}\")\n",
        "else:\n",
        "    print(f\"‚ùå Manifest not found at: {manifest_path}\")\n",
        "    raise FileNotFoundError(\"Upload training_manifest.json to Google Drive first!\")\n",
        "\n",
        "print(\"\\n‚úÖ All data files verified! Ready to proceed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load manifest\n",
        "with open(manifest_path) as f:\n",
        "    manifest = json.load(f)\n",
        "print(f\"Loaded manifest with {len(manifest)} cards\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_dataset(manifest, output_dir, train_ratio=0.85):\n",
        "    \"\"\"\n",
        "    Prepare dataset in class-folder structure.\n",
        "    Each card gets its own folder with its image.\n",
        "    \"\"\"\n",
        "    output_dir = Path(output_dir)\n",
        "    train_dir = output_dir / 'train'\n",
        "    val_dir = output_dir / 'val'\n",
        "    \n",
        "    # Clean up old data if exists\n",
        "    if output_dir.exists():\n",
        "        shutil.rmtree(output_dir)\n",
        "    \n",
        "    # Collect all card images\n",
        "    cards = []\n",
        "    missing = 0\n",
        "    \n",
        "    for entry in manifest:\n",
        "        # FIX: Convert Windows paths to Linux paths\n",
        "        img_rel_path = entry['image_path'].replace('\\\\', '/')\n",
        "        img_path = Path(DRIVE_PROJECT) / 'ml/data' / img_rel_path\n",
        "        \n",
        "        if img_path.exists():\n",
        "            cards.append({\n",
        "                'path': img_path,\n",
        "                'product_id': str(entry['product_id']),\n",
        "                'name': entry['clean_name'],\n",
        "            })\n",
        "        else:\n",
        "            missing += 1\n",
        "    \n",
        "    print(f\"Found {len(cards)} card images ({missing} missing)\")\n",
        "    \n",
        "    if len(cards) == 0:\n",
        "        raise ValueError(\"No cards found!\")\n",
        "    \n",
        "    # Shuffle and split\n",
        "    random.seed(42)\n",
        "    random.shuffle(cards)\n",
        "    \n",
        "    split_idx = int(len(cards) * train_ratio)\n",
        "    train_cards = cards[:split_idx]\n",
        "    val_cards = cards[split_idx:]\n",
        "    \n",
        "    print(f\"Split: {len(train_cards)} train, {len(val_cards)} val\")\n",
        "    \n",
        "    # Copy files to class folders\n",
        "    for split_name, split_cards, split_dir in [\n",
        "        ('train', train_cards, train_dir),\n",
        "        ('val', val_cards, val_dir)\n",
        "    ]:\n",
        "        print(f\"\\nPreparing {split_name} set...\")\n",
        "        for card in tqdm(split_cards, desc=split_name):\n",
        "            # Create class directory using product_id\n",
        "            class_dir = split_dir / card['product_id']\n",
        "            class_dir.mkdir(parents=True, exist_ok=True)\n",
        "            \n",
        "            # Sanitize filename\n",
        "            safe_name = \"\".join(c for c in card['name'] if c.isalnum() or c in ' _-').strip()[:50]\n",
        "            dst_path = class_dir / f\"{safe_name}.jpg\"\n",
        "            \n",
        "            if not dst_path.exists():\n",
        "                shutil.copy(card['path'], dst_path)\n",
        "    \n",
        "    # Verify\n",
        "    train_classes = len(list(train_dir.iterdir()))\n",
        "    val_classes = len(list(val_dir.iterdir()))\n",
        "    \n",
        "    print(f\"\\n‚úÖ Dataset prepared!\")\n",
        "    print(f\"   Train: {train_classes} classes\")\n",
        "    print(f\"   Val: {val_classes} classes\")\n",
        "    \n",
        "    return train_dir, val_dir\n",
        "\n",
        "# Prepare the dataset\n",
        "train_dir, val_dir = prepare_dataset(\n",
        "    manifest=manifest,\n",
        "    output_dir=Path(WORK_DIR) / 'data/embedding',\n",
        "    train_ratio=0.85\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Define Model & Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "import lightning as L\n",
        "import numpy as np\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from PIL import Image\n",
        "from pytorch_metric_learning import losses, miners\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Training configuration\n",
        "CONFIG = {\n",
        "    'model': {\n",
        "        'backbone': 'fastvit_t12',\n",
        "        'embedding_dim': 384,\n",
        "        'dropout': 0.2,\n",
        "    },\n",
        "    'training': {\n",
        "        'epochs': 100,\n",
        "        'batch_size': 64,\n",
        "        'learning_rate': 0.0003,\n",
        "        'weight_decay': 0.05,\n",
        "        'views_per_card': 4,  # Augmented views per card\n",
        "    },\n",
        "    'metric_learning': {\n",
        "        'margin': 0.5,\n",
        "        'scale': 64,\n",
        "    },\n",
        "}\n",
        "\n",
        "print(\"Configuration loaded:\")\n",
        "print(f\"  Backbone: {CONFIG['model']['backbone']}\")\n",
        "print(f\"  Embedding dim: {CONFIG['model']['embedding_dim']}\")\n",
        "print(f\"  Batch size: {CONFIG['training']['batch_size']}\")\n",
        "print(f\"  Views per card: {CONFIG['training']['views_per_card']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_train_transforms():\n",
        "    \"\"\"Heavy augmentation to simulate real-world conditions.\"\"\"\n",
        "    return A.Compose([\n",
        "        # Geometric transforms\n",
        "        A.Perspective(scale=(0.05, 0.12), p=0.5),\n",
        "        A.Affine(rotate=(-15, 15), shear=(-8, 8), scale=(0.85, 1.15), p=0.7),\n",
        "        \n",
        "        # Lighting variations\n",
        "        A.OneOf([\n",
        "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=1.0),\n",
        "            A.HueSaturationValue(hue_shift_limit=8, sat_shift_limit=25, val_shift_limit=25, p=1.0),\n",
        "            A.RandomGamma(gamma_limit=(70, 130), p=1.0),\n",
        "        ], p=0.6),\n",
        "        \n",
        "        # Blur (camera out of focus)\n",
        "        A.OneOf([\n",
        "            A.GaussianBlur(blur_limit=(3, 5), p=1.0),\n",
        "            A.MotionBlur(blur_limit=5, p=1.0),\n",
        "        ], p=0.3),\n",
        "        \n",
        "        # Noise (low light)\n",
        "        A.GaussNoise(var_limit=(5, 50), p=0.3),\n",
        "        \n",
        "        # JPEG compression artifacts\n",
        "        A.ImageCompression(quality_lower=70, quality_upper=100, p=0.3),\n",
        "        \n",
        "        # Resize and normalize\n",
        "        A.Resize(256, 256),\n",
        "        A.RandomCrop(224, 224),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "def get_val_transforms():\n",
        "    \"\"\"Simple transforms for validation/inference.\"\"\"\n",
        "    return A.Compose([\n",
        "        A.Resize(256, 256),\n",
        "        A.CenterCrop(224, 224),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CardDataset(Dataset):\n",
        "    \"\"\"Dataset that generates multiple augmented views per card.\"\"\"\n",
        "    \n",
        "    def __init__(self, root_dir, transform, views_per_card=1):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.transform = transform\n",
        "        self.views_per_card = views_per_card\n",
        "        \n",
        "        # Collect all images and their class indices\n",
        "        self.samples = []\n",
        "        self.class_to_idx = {}\n",
        "        self.idx_to_class = {}\n",
        "        \n",
        "        for idx, class_dir in enumerate(sorted(self.root_dir.iterdir())):\n",
        "            if class_dir.is_dir():\n",
        "                class_name = class_dir.name\n",
        "                self.class_to_idx[class_name] = idx\n",
        "                self.idx_to_class[idx] = class_name\n",
        "                \n",
        "                for img_path in class_dir.glob('*.jpg'):\n",
        "                    self.samples.append((img_path, idx))\n",
        "        \n",
        "        print(f\"Loaded {len(self.samples)} images, {len(self.class_to_idx)} classes\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.samples) * self.views_per_card\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Map idx to actual sample (allows multiple views)\n",
        "        sample_idx = idx // self.views_per_card\n",
        "        img_path, label = self.samples[sample_idx]\n",
        "        \n",
        "        # Load and transform image\n",
        "        image = np.array(Image.open(img_path).convert('RGB'))\n",
        "        augmented = self.transform(image=image)\n",
        "        \n",
        "        return augmented['image'], label\n",
        "    \n",
        "    @property\n",
        "    def num_classes(self):\n",
        "        return len(self.class_to_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EmbeddingModel(nn.Module):\n",
        "    \"\"\"FastViT backbone with embedding head.\"\"\"\n",
        "    \n",
        "    def __init__(self, backbone='fastvit_t12', embedding_dim=384, dropout=0.2):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Load pretrained backbone\n",
        "        self.backbone = timm.create_model(backbone, pretrained=True, num_classes=0)\n",
        "        \n",
        "        # Get feature dimension\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.randn(1, 3, 224, 224)\n",
        "            features = self.backbone(dummy)\n",
        "            feature_dim = features.shape[-1]\n",
        "        \n",
        "        # Embedding head\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(feature_dim, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, embedding_dim),\n",
        "        )\n",
        "        \n",
        "        print(f\"Model created: {backbone} -> {feature_dim} -> {embedding_dim}\")\n",
        "    \n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        embeddings = self.head(features)\n",
        "        # L2 normalize embeddings\n",
        "        return F.normalize(embeddings, p=2, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TrainingModule(L.LightningModule):\n",
        "    \"\"\"PyTorch Lightning training module.\"\"\"\n",
        "    \n",
        "    def __init__(self, config, num_classes):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.config = config\n",
        "        \n",
        "        # Model\n",
        "        self.model = EmbeddingModel(\n",
        "            backbone=config['model']['backbone'],\n",
        "            embedding_dim=config['model']['embedding_dim'],\n",
        "            dropout=config['model']['dropout'],\n",
        "        )\n",
        "        \n",
        "        # ArcFace loss for metric learning\n",
        "        self.loss_fn = losses.ArcFaceLoss(\n",
        "            num_classes=num_classes,\n",
        "            embedding_size=config['model']['embedding_dim'],\n",
        "            margin=config['metric_learning']['margin'],\n",
        "            scale=config['metric_learning']['scale'],\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, labels = batch\n",
        "        embeddings = self(images)\n",
        "        loss = self.loss_fn(embeddings, labels)\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        images, labels = batch\n",
        "        embeddings = self(images)\n",
        "        loss = self.loss_fn(embeddings, labels)\n",
        "        self.log('val_loss', loss, prog_bar=True, sync_dist=True)\n",
        "        return loss\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            self.parameters(),\n",
        "            lr=self.config['training']['learning_rate'],\n",
        "            weight_decay=self.config['training']['weight_decay'],\n",
        "        )\n",
        "        \n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer,\n",
        "            T_max=self.config['training']['epochs'],\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'optimizer': optimizer,\n",
        "            'lr_scheduler': {'scheduler': scheduler, 'interval': 'epoch'}\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Create Datasets & Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "print(\"Creating datasets...\\n\")\n",
        "\n",
        "train_dataset = CardDataset(\n",
        "    train_dir,\n",
        "    transform=get_train_transforms(),\n",
        "    views_per_card=CONFIG['training']['views_per_card'],\n",
        ")\n",
        "\n",
        "val_dataset = CardDataset(\n",
        "    val_dir,\n",
        "    transform=get_val_transforms(),\n",
        "    views_per_card=1,\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining samples: {len(train_dataset)} ({train_dataset.num_classes} cards √ó {CONFIG['training']['views_per_card']} views)\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataloaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=CONFIG['training']['batch_size'],\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=CONFIG['training']['batch_size'],\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Val batches: {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model and trainer\n",
        "model = TrainingModule(CONFIG, num_classes=train_dataset.num_classes)\n",
        "\n",
        "# Callbacks\n",
        "callbacks = [\n",
        "    L.pytorch.callbacks.ModelCheckpoint(\n",
        "        dirpath=Path(WORK_DIR) / 'checkpoints',\n",
        "        filename='best-{epoch:02d}-{val_loss:.4f}',\n",
        "        monitor='val_loss',\n",
        "        mode='min',\n",
        "        save_top_k=3,\n",
        "        save_last=True,\n",
        "    ),\n",
        "    L.pytorch.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=15,\n",
        "        mode='min',\n",
        "        verbose=True,\n",
        "    ),\n",
        "    L.pytorch.callbacks.LearningRateMonitor(logging_interval='epoch'),\n",
        "]\n",
        "\n",
        "# Trainer\n",
        "trainer = L.Trainer(\n",
        "    max_epochs=CONFIG['training']['epochs'],\n",
        "    accelerator='gpu',\n",
        "    devices=1,\n",
        "    precision='16-mixed',\n",
        "    callbacks=callbacks,\n",
        "    default_root_dir=Path(WORK_DIR) / 'logs',\n",
        "    log_every_n_steps=10,\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Ready to train!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# START TRAINING\n",
        "print(\"=\" * 50)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Training {train_dataset.num_classes} card classes\")\n",
        "print(f\"Max epochs: {CONFIG['training']['epochs']}\")\n",
        "print(f\"Early stopping patience: 15 epochs\")\n",
        "print(\"\")\n",
        "print(\"Monitor 'val_loss' - lower is better\")\n",
        "print(\"Training will stop automatically when val_loss stops improving\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "trainer.fit(model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "print(\"\\nSaving model...\")\n",
        "\n",
        "output_dir = Path(WORK_DIR) / 'output'\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Save model weights\n",
        "model_path = output_dir / 'embedding_model.pt'\n",
        "torch.save(model.model.state_dict(), model_path)\n",
        "print(f\"‚úÖ Model saved: {model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Generate Embeddings for All Cards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate embeddings for ALL cards (train + val)\n",
        "print(\"Generating embeddings for all cards...\\n\")\n",
        "\n",
        "model.eval()\n",
        "device = torch.device('cuda')\n",
        "model = model.to(device)\n",
        "\n",
        "# Simple transform for embedding generation\n",
        "embed_transform = get_val_transforms()\n",
        "\n",
        "all_embeddings = []\n",
        "all_product_ids = []\n",
        "\n",
        "data_dir = Path(WORK_DIR) / 'data/embedding'\n",
        "\n",
        "# Process all cards (train + val)\n",
        "for split in ['train', 'val']:\n",
        "    split_dir = data_dir / split\n",
        "    print(f\"Processing {split}...\")\n",
        "    \n",
        "    for class_dir in tqdm(sorted(split_dir.iterdir())):\n",
        "        if not class_dir.is_dir():\n",
        "            continue\n",
        "            \n",
        "        product_id = class_dir.name\n",
        "        \n",
        "        for img_path in class_dir.glob('*.jpg'):\n",
        "            # Load and transform\n",
        "            image = np.array(Image.open(img_path).convert('RGB'))\n",
        "            transformed = embed_transform(image=image)\n",
        "            img_tensor = transformed['image'].unsqueeze(0).to(device)\n",
        "            \n",
        "            # Generate embedding\n",
        "            with torch.no_grad():\n",
        "                embedding = model(img_tensor).cpu().numpy()[0]\n",
        "            \n",
        "            all_embeddings.append(embedding)\n",
        "            all_product_ids.append(product_id)\n",
        "\n",
        "# Convert to numpy array\n",
        "embeddings = np.array(all_embeddings)\n",
        "print(f\"\\n‚úÖ Generated {len(embeddings)} embeddings\")\n",
        "print(f\"   Shape: {embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Build Vector Search Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from annoy import AnnoyIndex\n",
        "\n",
        "print(\"Building Annoy index...\")\n",
        "\n",
        "# Create index\n",
        "embedding_dim = embeddings.shape[1]\n",
        "index = AnnoyIndex(embedding_dim, 'angular')  # angular = cosine similarity\n",
        "\n",
        "# Add all embeddings\n",
        "for i, emb in enumerate(embeddings):\n",
        "    index.add_item(i, emb)\n",
        "\n",
        "# Build with 10 trees (good balance of speed/accuracy)\n",
        "index.build(10)\n",
        "\n",
        "# Save locally\n",
        "index_path = output_dir / 'riftbound.ann'\n",
        "index.save(str(index_path))\n",
        "print(f\"‚úÖ Index saved: {index_path}\")\n",
        "print(f\"   Vectors: {len(embeddings)}\")\n",
        "print(f\"   Dimensions: {embedding_dim}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the index with a few queries\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"TESTING VECTOR INDEX\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Test 3 random cards\n",
        "test_indices = [0, len(embeddings) // 2, len(embeddings) - 1]\n",
        "\n",
        "for test_idx in test_indices:\n",
        "    neighbors, distances = index.get_nns_by_item(test_idx, 5, include_distances=True)\n",
        "    \n",
        "    query_id = all_product_ids[test_idx]\n",
        "    print(f\"\\nQuery: Product ID {query_id}\")\n",
        "    print(\"Top 5 matches:\")\n",
        "    \n",
        "    for rank, (n, d) in enumerate(zip(neighbors, distances), 1):\n",
        "        match_id = all_product_ids[n]\n",
        "        is_self = \"(self)\" if n == test_idx else \"\"\n",
        "        print(f\"  {rank}. ID: {match_id}, Distance: {d:.4f} {is_self}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Save Everything to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "print(\"Saving all outputs to Google Drive...\\n\")\n",
        "\n",
        "# Create directories\n",
        "drive_models = Path(DRIVE_PROJECT) / 'models/embedding'\n",
        "drive_indices = Path(DRIVE_PROJECT) / 'models/indices'\n",
        "drive_embeddings = Path(DRIVE_PROJECT) / 'ml/data/embeddings'\n",
        "\n",
        "drive_models.mkdir(parents=True, exist_ok=True)\n",
        "drive_indices.mkdir(parents=True, exist_ok=True)\n",
        "drive_embeddings.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 1. Copy model\n",
        "shutil.copy(output_dir / 'embedding_model.pt', drive_models / 'embedding_model.pt')\n",
        "print(f\"‚úÖ Model: {drive_models / 'embedding_model.pt'}\")\n",
        "\n",
        "# 2. Copy index\n",
        "shutil.copy(output_dir / 'riftbound.ann', drive_indices / 'riftbound.ann')\n",
        "print(f\"‚úÖ Index: {drive_indices / 'riftbound.ann'}\")\n",
        "\n",
        "# 3. Save embeddings\n",
        "np.save(drive_embeddings / 'riftbound_embeddings.npy', embeddings)\n",
        "print(f\"‚úÖ Embeddings: {drive_embeddings / 'riftbound_embeddings.npy'}\")\n",
        "\n",
        "# 4. Save product ID mapping\n",
        "with open(drive_embeddings / 'riftbound_product_ids.json', 'w') as f:\n",
        "    json.dump(all_product_ids, f)\n",
        "print(f\"‚úÖ Product IDs: {drive_embeddings / 'riftbound_product_ids.json'}\")\n",
        "\n",
        "# 5. Save config for reference\n",
        "with open(drive_models / 'config.json', 'w') as f:\n",
        "    json.dump(CONFIG, f, indent=2)\n",
        "print(f\"‚úÖ Config: {drive_models / 'config.json'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"                    TRAINING COMPLETE!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nFiles saved to Google Drive:\")\n",
        "print(f\"\\nüìÅ {DRIVE_PROJECT}/\")\n",
        "print(\"   ‚îú‚îÄ‚îÄ models/\")\n",
        "print(\"   ‚îÇ   ‚îú‚îÄ‚îÄ embedding/\")\n",
        "print(\"   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embedding_model.pt      <- PyTorch model\")\n",
        "print(\"   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.json             <- Training config\")\n",
        "print(\"   ‚îÇ   ‚îî‚îÄ‚îÄ indices/\")\n",
        "print(\"   ‚îÇ       ‚îî‚îÄ‚îÄ riftbound.ann           <- Vector search index\")\n",
        "print(\"   ‚îî‚îÄ‚îÄ ml/data/embeddings/\")\n",
        "print(\"       ‚îú‚îÄ‚îÄ riftbound_embeddings.npy    <- All card embeddings\")\n",
        "print(\"       ‚îî‚îÄ‚îÄ riftbound_product_ids.json  <- Product ID mapping\")\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"\\nNEXT STEPS:\")\n",
        "print(\"1. Download these files from Google Drive\")\n",
        "print(\"2. Convert to TFLite locally (I'll help with this)\")\n",
        "print(\"3. Add to Flutter app assets\")\n",
        "print(\"4. Test the app!\")\n",
        "print(\"\\n\" + \"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
